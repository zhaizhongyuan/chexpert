{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, splitext\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nnco\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, roc_curve, auc\n",
    "# from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../../../../../../storage/ice1/shared/bmed6780/mip_group_2/CheXpert Plus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the parent directory containing subdirectories (e.g., 'label_folder').\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            mode (str): Either \"train\" or \"valid\" to select the correct folder.\n",
    "        \"\"\"\n",
    "        \n",
    "        # self.label_folder = os.path.join(root_dir, 'chexbert_labels')\n",
    "        self.root = root_dir\n",
    "        self.img_path = os.path.join(self.root, 'PNG')\n",
    "        self.img_folders = [folder for folder in os.listdir(self.img_path) if splitext(folder)[1] == '']\n",
    "\n",
    "        self.label_folder = os.path.join(self.root, 'chexbert_labels')\n",
    "        self.label_path = os.path.join(self.label_folder, 'findings_fixed.json')\n",
    "        self.labels = []\n",
    "        self.img_paths = []\n",
    "        self.img_value_exception = 'train/patient32368/study1/view1_frontal.jpg'\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # load a dictionary of image paths and labels\n",
    "        with open(self.label_path, 'r') as f:\n",
    "            label_data = []\n",
    "            for line in f:\n",
    "                label_data.append(json.loads(line))\n",
    "\n",
    "        for label_dict in label_data:\n",
    "            label_list_per_sample = []\n",
    "            for key, value in label_dict.items():\n",
    "                if key == 'path_to_image': # save image paths\n",
    "                    if value != self.img_value_exception and splitext(value)[0].split('/')[0] == self.mode:\n",
    "                        for folder in self.img_folders:\n",
    "                            img_subfolder_path = os.path.join(os.path.join(self.img_path, folder), 'PNG')\n",
    "                            img_path = os.path.join(img_subfolder_path, value)\n",
    "                            if os.path.exists(img_path):\n",
    "                                self.img_paths.append(img_path)\n",
    "                    else:\n",
    "                        break # so labels for test data will not be saved\n",
    "                else: # save label vectors\n",
    "                    if value is None: \n",
    "                        label_list_per_sample.append(0) # if this disease is not mentioned, it is perhaps not present\n",
    "                    elif value == -1:\n",
    "                        label_list_per_sample.append(0.5) # if radiologist is uncertain, chances of having this disease or being healthy are half half\n",
    "                    else:\n",
    "                        label_list_per_sample.append(value) # either having this disease or not\n",
    "            if len(label_list_per_sample) > 0: # empty list implies a testing smaple\n",
    "                self.labels.append(torch.tensor(label_list_per_sample))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # convert to RGB\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = CheXpertDataset(root_dir=data_folder, transform=transform, mode=\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223228"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "model = densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, num_epochs=1, learning_rate=1e-3, weight_decay=1e-5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss() # multi-label classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 28 13:15:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H200                    On  |   00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             76W /  700W |       4MiB / 143771MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDenseNetMultiLabel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, lr=1e-3, weight_decay=1e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        weights = DenseNet121_Weights.DEFAULT\n",
    "        self.model = densenet121(weights=weights)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.train_acc = MultilabelAccuracy(num_labels=num_classes, threshold=0.5)\n",
    "        self.val_acc = MultilabelAccuracy(num_labels=num_classes, threshold=0.5)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.val_preds = []\n",
    "        self.val_targets = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y.float())\n",
    "        preds = torch.sigmoid(logits)\n",
    "        acc = self.train_acc(preds, y.int())\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y.float())\n",
    "        preds = torch.sigmoid(logits)\n",
    "        acc = self.val_acc(preds, y.int())\n",
    "\n",
    "        self.val_preds.append(preds.detach().cpu())\n",
    "        self.val_targets.append(y.detach().cpu())\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = torch.cat(self.val_preds)\n",
    "        targets = torch.cat(self.val_targets)\n",
    "\n",
    "        pred_labels = (preds > 0.5).int()\n",
    "\n",
    "        # log confusion matrix per class\n",
    "        cm = multilabel_confusion_matrix(targets, pred_labels)\n",
    "        for i in range(self.num_classes):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(cm[i], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            ax.set_title(f\"Confusion Matrix - Class {i}\")\n",
    "            ax.set_xlabel(\"Predicted\")\n",
    "            ax.set_ylabel(\"True\")\n",
    "            wandb.log({f\"confusion_matrix_class_{i}\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "\n",
    "        # log ROC curves\n",
    "        for i in range(self.num_classes):\n",
    "            fpr, tpr, _ = roc_curve(targets[:, i], preds[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "            ax.plot([0, 1], [0, 1], 'k--')\n",
    "            ax.set_title(f\"ROC Curve - Class {i}\")\n",
    "            ax.set_xlabel(\"False Positive Rate\")\n",
    "            ax.set_ylabel(\"True Positive Rate\")\n",
    "            ax.legend(loc=\"lower right\")\n",
    "            wandb.log({f\"roc_curve_class_{i}\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "\n",
    "        # clear stored predictions\n",
    "        self.val_preds.clear()\n",
    "        self.val_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiLabelDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = DenseNet121_Weights.DEFAULT.transforms()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = CheXpertDataset(root_dir=data_dir, transform=transform, mode=\"train\")\n",
    "        self.train_set, self.val_set = random_split(full_dataset, [0.8, 0.2])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/hice1/zzhai37/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[39m=\u001b[39m MultiLabelDataModule(data_dir\u001b[39m=\u001b[39mdata_folder)\n\u001b[1;32m      6\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      7\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     precision\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m     \u001b[39m# optional mixed precision\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    540\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:110\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Launches processes that run the given function in parallel.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39mThe function is allowed to have a return value. However, when all processes join, only the return value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_method \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mforkserver\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m     _check_bad_cuda_fork()\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     _check_missing_main_guard()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/strategies/launchers/multiprocessing.py:208\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    207\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m You will have to restart the Python kernel.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 208\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LitDenseNetMultiLabel(num_classes=14)\n",
    "data = MultiLabelDataModule(data_dir=data_folder)\n",
    "wandb_logger = WandbLogger(project=\"chexpert_multilabel\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='gpu',\n",
    "    devices='auto',  # or specify a list like devices=[0,1]\n",
    "    strategy='ddp',  # distributed data parallel\n",
    "    precision=16,     # optional mixed precision\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
